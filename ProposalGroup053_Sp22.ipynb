{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFHnriKpFNPN"
      },
      "source": [
        "# COGS 118A- Project Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0W1qM1KFNPR"
      },
      "source": [
        "# Names\n",
        "\n",
        "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
        "\n",
        "- Erlend Hordvei\n",
        "- Adam Alvord\n",
        "- Ethan Black\n",
        "- Cedric Wong\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8cCIr4nFNPR"
      },
      "source": [
        "# Abstract \n",
        "\n",
        "Our project aims to create a supervised machine learning algorithm that can, with decent accuracy, predict what language a novel word comes from based on its phonemes. We have a dataset of words in 28 different languages and their translations into the International Phonetic Alphabet (IPA). We will train our algorithm with a subset of these words so it can learn the phonetic inventory of each of our languages. Then, we will feed it novel words and see how well we can predict their origin. In order to accomplish this, we will clean and organize our datasets into one matrix that keeps a count of the phonemes used in each individual word, and one-hot encodes its language of origin.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5njEdSsFNPR"
      },
      "source": [
        "# Background\n",
        "\n",
        "Identification of natural languages is a topic that has undergone many studies in the past. This task is often trivial for human translators, but coding a computer to do this has a much higher difficulty, and even human translators are known to make mistakes when it comes to obscure languages <a name=\"beesley\"></a>[<sup>[1]</sup>](#beesleynote). There are a few different possible approaches. First is to match the words by letters, and their presence in each language. Alternatively, it can be done matching entire words that appear in multiple languages <a name=\"beesley\"></a>[<sup>[1]</sup>](#beesleynote). Souter's article suggests using combinations of two or three letters, having the algorithm split each word <a name=\"souter\"></a>[<sup>[2]</sup>](#souternote).\n",
        "\n",
        "The data we will be using is already conveniently separated into phonemes, which lowers the amount of cleaning we will need to simple reshaping and merging of the data. There will likely be less regex text processing than is done in other research on this topic <a name=\"saji\"></a>[<sup>[3]</sup>](#sajinote). We'll also split our training and test data in an 80-20 ratio like the experiment by Saji. It remains to be seen which model we will use, but there are several options depending on what kind of comparisons we would like to make."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTeuMmJnFNPR"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "Identification of languages can be exceptionally difficult for humans who do not speak them. Humans excel in discerning speech-sounds from their native language’s phonetic inventory since they have been trained to do so since birth, but this comes at a cost. It’s often difficult for speakers of a given language to distinguish between, or to produce, phonemes that are outside of their phonetic inventory. This project aims to assist in language recognition using IPA and statistical, supervised machine learning. In order to do so, we must have our algorithm learn the phonetic inventories of our 28 different languages, and be able to calculate the probability of a novel word’s origin based on the phonemes it contains, their frequency in the word, and their frequency in a given language. One potential issue with this, however, is that not all of our language sets are the same size, meaning some languages will be over or under represented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slKKNGERFNPS"
      },
      "source": [
        "# Data\n",
        "\n",
        "ipa-dict:\n",
        "*   https://github.com/open-dict-data/ipa-dict/releases\n",
        "*   The data is organized into 28 files containing one language dictionary, so we will count the categorical information of what language dictionary each word belongs to as a variable. From there, we plan on encoding each word as a frequency count of each phoneme, where a column of data represents a phoneme, and each row represents a word/observation. As such, we will need a number of columns equal to the combined phoneme inventory of all 28 languages in the dataset. There are around 4 million words (3,890,537) in the dataset.\n",
        "An observation will be an individual word in one of the dictionaries.\n",
        "*   Variable Representation:\n",
        "   *  We will have a one-hot encoded categorical variable representing the dictionary of origin for each observation.\n",
        "   *  We will have some number of variables representing phoneme frequency data for each observation. The number of variables required is equivalent to the number of phonemes represented in all 28 languages.\n",
        "*  The ipa-dict database contains dictionaries extracted from a number of different sources. It is possible there will be some incompatibilities between the formats of each dictionary we may need to resolve. We need to parse the IPA characters in the files and produce frequency counts for each word to represent them in our dataset, and assign the categorical language data based on the file we’re extracting each word from.\n",
        "\n",
        "Some of the dictionary files are significantly different in size, because many languages are linguistically under-documented.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCy-VodKFNPS"
      },
      "source": [
        "# Proposed Solution\n",
        "\n",
        "In order to produce a model which can discriminate between different languages, we’re going to need a classification model. We haven’t yet covered multiclass classification algorithms, so we’re not entirely sure what form this model will take as of yet, but we know that we definitely need a multiclass classifier to manage identifying 28 languages. Intuitively, a variant of the Decision Tree makes sense to us, some of these languages have phonemes that may be exclusive to their inventory and a decision tree seems like it would handle that pretty well. For testing, we’re going to reserve a set of novel words from each language to test our model’s accuracy, using a proportional 20% of each language dictionary to ensure our test set evaluates accuracy including as much of the low data language dictionaries as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW4PSzIdFNPT"
      },
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "We will evaluate our model based on its testing accuracy. We will take a score of both how accurately we can generally classify novel words (i.e. how often we get the correct categorization), as well as accuracy across individual languages or groups of languages. This way we can see trends between how often we can predict the exact language of origin, as well as how well we can discern linguistic groups like latin-descended or germanic-descended languages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHTu38GWFNPT"
      },
      "source": [
        "# Ethics & Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjYSQUHdFNPV"
      },
      "source": [
        "While the general function of this project may not in itself be objectionable, language dictionaries do not breach the privacy of any particular individual, the structural problems with how this sort of data is gathered were clear to us in searching for datasets to use. Centralized resources for phoneme information are fragmented, incomplete, and biased towards large languages on the whole, and that means, regardless of intent, tools developed for this functionality will disproportionately benefit those with the research apparatus in place to conduct linguistic research. \n",
        "\n",
        "In other words, underserved cultural and linguistic communities, without the wealth to conduct language research, are unlikely to benefit from these ML tools without outside funds and resources to support data gathering for this purpose. Even finding non-English IPA datasets was a bit of a challenge. Generally, an overemphasis on European interests seems to be foundationally baked into the available language datasets, and that overemphasis will likely carry over to our project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4zlu5XJFNPV"
      },
      "source": [
        "# Team Expectations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1mcbGC7FNPV"
      },
      "source": [
        "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
        "* Communicate scheduling changes ASAP\n",
        "  *  We will communicate mainly through our group discord\n",
        "* Accept a realistic workload. Don’t take on too much!\n",
        "  *  Taking on too much work at once will cause burnout, and can cause us to miss important milestones or deadlines. Don’t be afraid to ask for help!\n",
        "* Honor each other’s time and work\n",
        "  *  We’ll try our best to be punctual and work around each other’s schedules, as well as meet deadlines we set together as a group\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc-dh1C2FNPV"
      },
      "source": [
        "# Project Timeline Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS7YeSZsFNPV"
      },
      "source": [
        "Our group will have weekly meetings to set goals, delegate work, and check in about deadlines. We’ve created a when2meet to coordinate this, and we’ll make adjustments to accommodate work/school schedules as needed throughout the rest of the quarter.\n",
        "\n",
        "\n",
        "Our first objective will be to preprocess our datasets and get them into workable form. Then we’ll review it together and discuss our hypothesis, then do some background research individually. Afterwards, we’ll do some EDA and figure out the best model to create for our project.\n",
        "\n",
        "\n",
        "Once we have our model picked out, it will be time to implement it. After running our data through and getting some initial test results we can make any necessary tweaks before another round of testing. Finally, we can begin our analysis and work on our final project presentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wZdoYJ7FNPV"
      },
      "source": [
        "# Footnotes\n",
        "<a name=\"beesleynote\"></a>1.[^](#beesley): Beesley, Kenneth. (1999). Language Identifier: A Computer Program for Automatic Natural-Language Identification of On-line Text. https://www.researchgate.net/publication/2391981_Language_Identifier_A_Computer_Program_for_Automatic_Natural-Language_Identification_of_On-line_Text\n",
        "<br> \n",
        "<a name=\"souternote\"></a>2.[^](#souter): Souter, C., Churcher, G., Hayes, J., Hughes, J., & Johnson, S. (1994). Natural Language Identification using Corpus- Based Models. Tisskrift.dk. Retrieved April 25, 2022, from https://tidsskrift.dk/her/article/download/25083/22006\n",
        "<br>\n",
        "<a name=\"sajinote\"></a>3.[^](#saji): Saji, B. (2021, March 12). Language detection using natural language processing. Analytics Vidhya. Retrieved April 24, 2022, from https://www.analyticsvidhya.com/blog/2021/03/language-detection-using-natural-language-processing/ \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ProposalGroup053-Sp22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}